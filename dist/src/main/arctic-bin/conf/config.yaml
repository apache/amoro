ams:
  admin-username: admin
  admin-password: admin
  server-bind-host: "0.0.0.0"
  server-expose-host: "127.0.0.1"

  thrift-server:
    max-message-size: 104857600 # 100MB
    selector-thread-count: 2
    selector-queue-size: 4
    table-service:
      bind-port: 1260
      worker-thread-count: 20
    optimizing-service:
      bind-port: 1261

  http-server:
    bind-port: 1630

  refresh-external-catalogs:
    interval: 180000 # 3min

  refresh-tables:
    thread-count: 10
    interval: 60000 # 1min
    
  self-optimizing:
    commit-thread-count: 10

  optimizer:
    heart-beat-timeout: 60000 # 1min
    task-ack-timeout: 30000 # 30s

  blocker:
    timeout: 60000 # 1min

  # optional features
  expire-snapshots:
    enabled: true
    thread-count: 10

  clean-orphan-files:
    enabled: true
    thread-count: 10

  sync-hive-tables:
    enabled: true
    thread-count: 10

  data-expiration:
    enabled: false
    thread-count: 10
    interval: 1d

  database:
    type: derby
    jdbc-driver-class: org.apache.derby.jdbc.EmbeddedDriver
    url: jdbc:derby:/tmp/amoro/derby;create=true

#    MySQL database configuration.
#    database:
#      type: mysql
#      jdbc-driver-class: com.mysql.cj.jdbc.Driver
#      url: jdbc:mysql://127.0.0.1:3306/db?useUnicode=true&characterEncoding=UTF8&autoReconnect=true&useAffectedRows=true&useSSL=false
#      username: root
#      password: root

  #  Postgres database configuration.
  #  database:
  #    type: postgres
  #    jdbc-driver-class: org.postgresql.Driver
  #    url: jdbc:postgresql://127.0.0.1:5432/db
  #    username: user
  #    password: passwd

  terminal:
    backend: local
    local.spark.sql.iceberg.handle-timestamp-without-timezone: false

#  Kyuubi terminal backend configuration.
#  terminal:
#    backend: kyuubi
#    kyuubi.jdbc.url: jdbc:hive2://127.0.0.1:10009/


#  High availability configuration.
#  ha:
#    enabled: true
#    cluster-name: default
#    zookeeper-address: 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183

containers:
  - name: localContainer
    container-impl: com.netease.arctic.optimizer.container.LocalOptimizerContainer
    properties:
      export.JAVA_HOME: "/opt/java"   # JDK environment

#containers:
#  - name: flinkContainer
#    container-impl: com.netease.arctic.optimizer.container.FlinkOptimizerContainer
#    properties:
#      flink-home: "/opt/flink/"                                     # Flink install home
#      target: "yarn-per-job"                                        # Flink run target, (yarn-per-job, yarn-application, kubernetes-application)
#      export.JVM_ARGS: "-Djava.security.krb5.conf=/opt/krb5.conf"   # Flink launch jvm args, like kerberos config when ues kerberos
#      export.HADOOP_CONF_DIR: "/etc/hadoop/conf/"                   # Hadoop config dir
#      export.HADOOP_USER_NAME: "hadoop"                             # Hadoop user submit on yarn
#      export.FLINK_CONF_DIR: "/opt/flink/conf/"                     # Flink config dir
#      # flink kubernetes application properties.
#      job-uri: "local:///opt/flink/usrlib/OptimizeJob.jar"          # optimizer job main jor for application mode
#      flink-conf.kubernetes.container.image: "arctic163/optimizer-flink1.14"   #optimizer image ref
#      flink-conf.kubernetes.service-account="flink"                 # service account to manager application job.
