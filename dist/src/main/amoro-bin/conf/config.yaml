#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

ams:
  admin-username: admin
  admin-password: admin
  server-bind-host: "0.0.0.0"
  server-expose-host: "127.0.0.1"

  thrift-server:
    max-message-size: 104857600 # 100MB
    selector-thread-count: 2
    selector-queue-size: 4
    table-service:
      bind-port: 1260
      worker-thread-count: 20
    optimizing-service:
      bind-port: 1261

  http-server:
    bind-port: 1630
    rest-auth-type: token

  refresh-external-catalogs:
    interval: 180000 # 3min
    thread-count: 10
    queue-size: 1000000

  refresh-tables:
    thread-count: 10
    interval: 60000 # 1min

  self-optimizing:
    commit-thread-count: 10
    runtime-data-keep-days: 30
    runtime-data-expire-interval-hours: 1

  optimizer:
    heart-beat-timeout: 60000 # 1min
    task-ack-timeout: 30000 # 30s
    polling-timeout: 3000 # 3s
    max-planning-parallelism: 1 # default 1

  blocker:
    timeout: 60000 # 1min

  # optional features
  expire-snapshots:
    enabled: true
    thread-count: 10

  clean-orphan-files:
    enabled: true
    thread-count: 10
    interval: 1d

  clean-dangling-delete-files:
    enabled: true
    thread-count: 10

  sync-hive-tables:
    enabled: false
    thread-count: 10

  data-expiration:
    enabled: true
    thread-count: 10
    interval: 1d

  auto-create-tags:
    enabled: true
    thread-count: 3
    interval: 60000 # 1min

  table-manifest-io:
    thread-count: 20

  database:
    type: derby
    jdbc-driver-class: org.apache.derby.jdbc.EmbeddedDriver
    url: jdbc:derby:/tmp/amoro/derby;create=true
    connection-pool-max-total: 20
    connection-pool-max-idle: 16
    connection-pool-max-wait-millis: 1000

#  MySQL database configuration.
#  database:
#    type: mysql
#    jdbc-driver-class: com.mysql.cj.jdbc.Driver
#    url: jdbc:mysql://127.0.0.1:3306/db?useUnicode=true&characterEncoding=UTF8&autoReconnect=true&useAffectedRows=true&allowPublicKeyRetrieval=true&useSSL=false
#    username: root
#    password: root
#    auto-create-tables: true
#    connection-pool-max-total: 20
#    connection-pool-max-idle: 16
#    connection-pool-max-wait-millis: 1000

#  Postgres database configuration.
#  database:
#    type: postgres
#    jdbc-driver-class: org.postgresql.Driver
#    url: jdbc:postgresql://127.0.0.1:5432/db
#    username: user
#    password: passwd
#    auto-create-tables: true
#    connection-pool-max-total: 20
#    connection-pool-max-idle: 16
#    connection-pool-max-wait-millis: 1000

  terminal:
    backend: local
    result:
      limit: 1000
    stop-on-error: false
    session:
      timeout: 30
    local:
      using-session-catalog-for-hive: false
      spark.sql.iceberg.handle-timestamp-without-timezone: false

#  Kyuubi terminal backend configuration.
#  terminal:
#    backend: kyuubi
#    kyuubi.jdbc.url: jdbc:hive2://127.0.0.1:10009/

#  High availability configuration.
#  ha:
#    enabled: true
#    cluster-name: default
#    zookeeper-address: 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183


containers:
  - name: localContainer
    container-impl: org.apache.amoro.server.manager.LocalOptimizerContainer
    properties:
      export.JAVA_HOME: "/opt/java"   # JDK environment

#containers:

# - name: KubernetesContainer
#   container-impl: org.apache.amoro.server.manager.KubernetesOptimizerContainer
#    properties:
#     kube-config-path: ~/.kube/config
#     image: apache/amoro:{version}
#     namespace: default

#  - name: flinkContainer
#    container-impl: org.apache.amoro.server.manager.FlinkOptimizerContainer
#    properties:
#      flink-home: /opt/flink/                                     # Flink install home
#      target: yarn-per-job                                        # Flink run target, (yarn-per-job, yarn-application, kubernetes-application)
#      export.JVM_ARGS: -Djava.security.krb5.conf=/opt/krb5.conf   # Flink launch jvm args, like kerberos config when ues kerberos
#      export.HADOOP_CONF_DIR: /etc/hadoop/conf/                   # Hadoop config dir
#      export.HADOOP_USER_NAME: hadoop                             # Hadoop user submit on yarn
#      export.FLINK_CONF_DIR: /opt/flink/conf/                     # Flink config dir
#      # flink kubernetes application properties.
#      job-uri: "local:///opt/flink/usrlib/optimizer-job.jar"      # Optimizer job main jar for kubernetes application
#      flink-conf.kubernetes.container.image: "apache/amoro-flink-optimizer:{version}"   # Optimizer image ref
#      flink-conf.kubernetes.service-account: flink                # Service account that is used within kubernetes cluster.

#containers:
#  - name: sparkContainer
#    container-impl: org.apache.amoro.server.manager.SparkOptimizerContainer
#    properties:
#      spark-home: /opt/spark/                                     # Spark install home
#      master: yarn                                                # The cluster manager to connect to. See the list of https://spark.apache.org/docs/latest/submitting-applications.html#master-urls.
#      deploy-mode: cluster                                        # Spark deploy mode, client or cluster
#      export.JVM_ARGS: -Djava.security.krb5.conf=/opt/krb5.conf   # Spark launch jvm args, like kerberos config when ues kerberos
#      export.HADOOP_CONF_DIR: /etc/hadoop/conf/                   # Hadoop config dir
#      export.HADOOP_USER_NAME: hadoop                             # Hadoop user submit on yarn
#      export.SPARK_CONF_DIR: /opt/spark/conf/                     # Spark config dir
#      # spark kubernetes application properties.
#      job-uri: "local:///opt/spark/usrlib/optimizer-job.jar"      # Optimizer job main jar for kubernetes application
#      ams-optimizing-uri: thrift://ams.amoro.service.local:1261   # AMS optimizing uri
#      spark-conf.spark.dynamicAllocation.enabled: "true"          # Enabling DRA feature can make full use of computing resources
#      spark-conf.spark.shuffle.service.enabled: "false"           # If spark DRA is used on kubernetes, we should set it false
#      spark-conf.spark.dynamicAllocation.shuffleTracking.enabled: "true"                          # Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service
#      spark-conf.spark.kubernetes.container.image: "apache/amoro-spark-optimizer:{version}"       # Optimizer image ref
#      spark-conf.spark.kubernetes.namespace: <spark-namespace>                                    # Namespace that is used within kubernetes cluster
#      spark-conf.spark.kubernetes.authenticate.driver.serviceAccountName: <spark-sa>              # Service account that is used within kubernetes cluster.
