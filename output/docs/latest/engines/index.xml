<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Engines on Amoro</title>
    <link>https://amoro.apache.org/docs/latest/engines/</link>
    <description>Recent content in Engines on Amoro</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://amoro.apache.org/docs/latest/engines/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flink CDC Ingestion</title>
      <link>https://amoro.apache.org/docs/latest/flink-cdc-ingestion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-cdc-ingestion/</guid>
      <description>Apache CDC Ingestion CDC stands for Change Data Capture, which is a broad concept, as long as it can capture the change data, it can be called CDC. Flink CDC is a Log message-based data capture tool, all the inventory and incremental data can be captured. Taking MySQL as an example, it can easily capture Binlog data through Debezium and process the calculations in real time to send them to the data lake.</description>
    </item>
    <item>
      <title>Flink DataStream</title>
      <link>https://amoro.apache.org/docs/latest/flink-datastream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-datastream/</guid>
      <description>Flink DataStream Add maven dependency To add a dependency on Mixed-format flink connector in Maven, add the following to your pom.xml:&#xA;&amp;lt;dependencies&amp;gt; ... &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.amoro&amp;lt;/groupId&amp;gt; &amp;lt;!-- For example: amoro-format-mixed-flink-runtime-1.15 --&amp;gt; &amp;lt;artifactId&amp;gt;amoro-format-mixed-flink-runtime-${flink.minor-version}&amp;lt;/artifactId&amp;gt; &amp;lt;!-- For example: 0.7.0-incubating --&amp;gt; &amp;lt;version&amp;gt;${amoro-format-mixed-flink.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; ... &amp;lt;/dependencies&amp;gt; Reading with DataStream Amoro supports reading data in Batch or Streaming mode through Java API.&#xA;Batch mode Using Batch mode to read the full and incremental data in the FileStore.</description>
    </item>
    <item>
      <title>Flink DDL</title>
      <link>https://amoro.apache.org/docs/latest/flink-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-ddl/</guid>
      <description>Flink DDL Create catalogs Flink SQL The following statement can be executed to create a Flink catalog:&#xA;CREATE CATALOG &amp;lt;catalog_name&amp;gt; WITH ( &amp;#39;type&amp;#39;=&amp;#39;mixed_iceberg&amp;#39;, `&amp;lt;config_key&amp;gt;`=`&amp;lt;config_value&amp;gt;` ); Where &amp;lt;catalog_name&amp;gt; is the user-defined name of the Flink catalog, and &amp;lt;config_key&amp;gt;=&amp;lt;config_value&amp;gt; has the following configurations:&#xA;Key Default Value Type Required Description type N/A String Yes Catalog type, validate values are mixed_iceberg and mixed_hive ams.uri (none) String No The URI for Amoro Metastore is thrift://&amp;lt;ip&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;catalog_name_in_metastore&amp;gt;.</description>
    </item>
    <item>
      <title>Flink DML</title>
      <link>https://amoro.apache.org/docs/latest/flink-dml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-dml/</guid>
      <description>Flink DML Querying with SQL Amoro tables support reading data in stream or batch mode through Flink SQL. You can switch modes using the following methods:&#xA;-- Run Flink tasks in streaming mode in the current session SET execution.runtime-mode = streaming; -- Run Flink tasks in batch mode in the current session SET execution.runtime-mode = batch; The following Hint Options are supported:&#xA;Key Default Value Type Required Description source.parallelism (none) Integer No Defines a custom parallelism for the source.</description>
    </item>
    <item>
      <title>Flink Getting Started</title>
      <link>https://amoro.apache.org/docs/latest/flink-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-getting-started/</guid>
      <description>Flink Getting Started Iceberg format The Iceberg Format can be accessed using the Connector provided by Iceberg. Refer to the documentation at Iceberg Flink user manual for more information.&#xA;Paimon format The Paimon Format can be accessed using the Connector provided by Paimon. Refer to the documentation at Paimon Flink user manual for more information.&#xA;Mixed format The Apache Flink engine can process Amoro table data in batch and streaming mode.</description>
    </item>
    <item>
      <title>Spark Configuration</title>
      <link>https://amoro.apache.org/docs/latest/spark-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-configuration/</guid>
      <description>Spark Configuration Catalogs configuration Using Mixed-Format in a standalone catalog Starting from version 3.x, Spark supports configuring an independent Catalog. If you want to use a Mixed-Format table in a standalone Catalog, you can create a mixed_catalog and load catalog metadata from AMS with following properties:&#xA;spark.sql.catalog.mixed_catalog=org.apache.amoro.spark.MixedFormatSparkCatalog spark.sql.catalog.mixed_catalog.ams.uri=thrift://${AMS_HOST}:${AMS_PORT}/${AMS_CATALOG_NAME_HIVE} Or create a mixed_catalog with local configurations with following properties:&#xA;spark.sql.catalog.mixed_catalog=org.apache.amoro.spark.MixedFormatSparkCatalog # Configure mixed catalog type as you needed spark.sql.catalog.mixed_catalog.type=hadoop spark.sql.catalog.mixed_catalog.warehouse=/warehouse/hadoop_mixed_catalog Then, execute the following SQL in the Spark SQL Client to switch to the corresponding catalog.</description>
    </item>
    <item>
      <title>Spark DDL</title>
      <link>https://amoro.apache.org/docs/latest/spark-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-ddl/</guid>
      <description>Spark DDL CREATE TABLE To create an MixedFormat table under an Amoro Catalog, you can use using mixed_iceberg or using mixed_hive to specify the provider in the CREATE TABLE statement. If the Catalog type is Hive, the created table will be a Hive-compatible table.&#xA;CREATE TABLE mixed_catalog.db.sample ( id bigint COMMENT &amp;#34;unique id&amp;#34;, data string ) USING mixed_iceberg PRIMARY KEY You can use PRIMARY KEY in the CREATE TABLE statement to specify the primary key column.</description>
    </item>
    <item>
      <title>Spark Getting Started</title>
      <link>https://amoro.apache.org/docs/latest/spark-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-getting-started/</guid>
      <description>Spark Getting Started Iceberg Format The Iceberg Format can be accessed using the Connector provided by Iceberg. Refer to the documentation at Iceberg Spark Connector for more information.&#xA;Paimon Format The Paimon Format can be accessed using the Connector provided by Paimon. Refer to the documentation at Paimon Spark Connector for more information.&#xA;Mixed Format To use Amoro in a Spark shell, use the &amp;ndash;packages option:&#xA;spark-shell --packages org.apache.amoro:amoro-mixed-spark-3.3-runtime:0.7.0 If you want to include the connector in your Spark installation, add the amoro-mixed-spark-3.</description>
    </item>
    <item>
      <title>Spark Queries</title>
      <link>https://amoro.apache.org/docs/latest/spark-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-queries/</guid>
      <description>Spark Queries Querying with SQL Querying Mixed-Format table by merge on read Using Select statement to query on Mixed-Format tables.&#xA;SELECT * FROM mixed_catalog.db.sample The Mixed-Format connector will merge the data from BaseStore and ChangeStore.&#xA;Query on change store For a Mixed-Format table with primary keys. you can query on ChangeStore by .change.&#xA;SELECT * FROM mixed_catalog.db.sample.change +---+----+----+---------------+------------+--------------+ | id|name|data|_transaction_id|_file_offset|_change_action| +---+----+----+---------------+------------+--------------+ | 1|dddd|abcd| 3| 1| INSERT| | 1|dddd|abcd| 3| 2| DELETE| +---+----+----+---------------+------------+--------------+ The addition columns are:</description>
    </item>
    <item>
      <title>Spark Writes</title>
      <link>https://amoro.apache.org/docs/latest/spark-writes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-writes/</guid>
      <description>Spark Writes Writing with SQL INSERT OVERWRITE INSERT OVERWRITE can replace the partition in a table with the results of a query.&#xA;The default overwrite mode of Spark is Static, you can change the overwrite mode by&#xA;SET spark.sql.sources.partitionOverwriteMode=dynamic To demonstrate the behavior of dynamic and static overwrites, a test table is defined using the following DDL:&#xA;CREATE TABLE mixed_catalog.db.sample ( id int, data string, ts timestamp, primary key (id)) USING mixed_iceberg PARTITIONED BY (days(ts)) When Spark&amp;rsquo;s overwrite mode is dynamic, the partitions of the rows generated by the SELECT query will be replaced.</description>
    </item>
    <item>
      <title>Trino</title>
      <link>https://amoro.apache.org/docs/latest/trino/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/trino/</guid>
      <description>Trino Iceberg format Iceberg format can be accessed using the Iceberg Connector provided by Trino. please refer to the documentation at Iceberg Trino user manual for more information.&#xA;Paimon format Paimon format can be accessed using the Paimon Connector provided by Trino. please refer to the documentation at Paimon Trino user manual for more information.&#xA;Mixed format Install Create the {trino_home}/plugin/amoro directory in the Trino installation package, and extract the contents of the amoro-trino package amoro-mixed-trino-xx.</description>
    </item>
    <item>
      <title>Using Logstore</title>
      <link>https://amoro.apache.org/docs/latest/flink-using-logstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-using-logstore/</guid>
      <description>Using Logstore Due to the limitations of traditional offline data warehouse architectures in supporting real-time business needs, real-time data warehousing has experienced rapid evolution in recent years. In the architecture of real-time data warehousing, Apache Kafka is often used as the storage system for real-time data. However, this also brings about the issue of data disconnection between offline data warehouses.&#xA;Developers often need to pay attention to data stored in HDFS as well as data in Kafka, which increases the complexity of business development.</description>
    </item>
  </channel>
</rss>
